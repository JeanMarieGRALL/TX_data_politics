donnees[1]$authorName
donnees <- base[,.(nb_commentaire = .N), by = authorName]
donnees <- donnees[order(-nb_commentaire),]
donnees[1]$authorName
mean(donnees$nb_commentaire)
## 1.e Créer une variable avec la longueur du commentaire (fonction "nchar")
base$long_com = nchar(base$text)
##     Quelle est la longeur moyenne des commentaires?
mean(base$long_com)
##     Pensez-vous qu'il faut Supprimer les commentaires trop courts?
# oui
base <- base[long_com >= 9]
### II. CREATION ET EXPLORATION DU CORPUS : LEXICOMETRIE
cp <- corpus(base$text,
docvars = select(base, authorName, likeCount))
##     Quelle est la longeur moyenne des commentaires?
mean(base$long_com) # 187.3867
tk<-tokens(cp, what = "word", remove_punct = TRUE, remove_numbers= T, remove_url=T)
dfm<-dfm(cp, remove=stopwords("french"), tolower=T, remove_punct=T, remove_numbers=T, remove_url = T)
topfeatures(dfm, n=100)
## 2.b Enrichir la liste des stopwords,
## puis créer à nouveau la dfm et classer les mots les plus fréquents.
dfm <- dfm_remove(dfm,c("a","c'est","ça","si"))
## 2.c Repérer les expressions les plus courantes et construire un dictionnaire
textstat_collocations(tk, min_count = 5, size = 2L) %>%
arrange(desc(count)) %>%
slice(1:50)
base <- read_delim("C:/Users/brieu/Desktop/Cours/IC05/TD08/Comments.csv",
delim = ";", escape_double = FALSE, locale = locale(encoding = "WINDOWS-1252"),
trim_ws = TRUE)
## 1.b Transformer la colonne "PublishedAt" pour avoir deux variables "Date" et "Heure"
base$Date = str_sub(base$publishedAt,1,10)
base$Heure = str_sub(base$publishedAt,12,16)
##     Transformer la variable Date au format Date en utilisant le code suivant
base$Date <- as.Date(base$Date, "%d/%m/%y")
## 1.c Afficher l'évolution du nombre de commentaires publiés par jour
##     Comment expliquer les pics ?
setDT(base)
ggplot(base, aes(Date)) + geom_bar()
##     Retirer les doublons et les trolls.
base <- distinct(base,authorName,text, .keep_all = TRUE)
## 1.d Déterminer quel compte a publié le plus de commentaires
donnees <- base[,.(nb_commentaire = .N), by = authorName]
donnees <- donnees[order(-nb_commentaire),]
donnees[1]$authorName #  "Dominique Descottes"
##     Combien de commentaires les gens publient-ils en moyenne
mean(donnees$nb_commentaire) # 2.041616
## 1.e Créer une variable avec la longueur du commentaire (fonction "nchar")
base$long_com = nchar(base$text)
##     Quelle est la longeur moyenne des commentaires?
mean(base$long_com) # 187.3867
##     Pensez-vous qu'il faut Supprimer les commentaires trop courts?
# oui
### II. CREATION ET EXPLORATION DU CORPUS : LEXICOMETRIE
cp <- corpus(base$text,
docvars = select(base, authorName, likeCount))
## 2.a Créer les objets tokens et la matrice termes-documents
##     Classer les mots les plus fréquents : que constatez vous?
tk<-tokens(cp, what = "word", remove_punct = TRUE, remove_numbers= T, remove_url=T)
dfm<-dfm(cp, remove=stopwords("french"), tolower=T, remove_punct=T, remove_numbers=T, remove_url = T)
topfeatures(dfm, n=100)
## 2.b Enrichir la liste des stopwords,
## puis créer à nouveau la dfm et classer les mots les plus fréquents.
dfm <- dfm_remove(dfm,c("a","c'est","ça","si"))
topfeatures(dfm, n=100)
## 2.c Repérer les expressions les plus courantes et construire un dictionnaire
textstat_collocations(tk, min_count = 5, size = 2L) %>%
arrange(desc(count)) %>%
slice(1:50)
textplot_wordcloud(dfm,
min_size = 1,
max_size = 4,
min_count = 10,
max_words = 500,
rotation = 0.1,
color = "darkblue",
ordered_color = T)
## 2.d Mots clés dans leur contexte (fonction kwic() de la librairie quanteda)
##    A quels mots sont associés les termes "police/policier" ?
##                                                 "Castaner" ?
##                                             "Gouvernement" ?
Macron<-as.data.frame(kwic(cp, pattern= "macron", window=10))
base <- read_delim("C:/Users/brieu/Desktop/Cours/IC05/TD08/Comments.csv",
delim = ";", escape_double = FALSE, locale = locale(encoding = "WINDOWS-1252"),
trim_ws = TRUE)
## 1.b Transformer la colonne "PublishedAt" pour avoir deux variables "Date" et "Heure"
base$Date = str_sub(base$publishedAt,1,10)
base$Heure = str_sub(base$publishedAt,12,16)
##     Transformer la variable Date au format Date en utilisant le code suivant
base$Date <- as.Date(base$Date, "%d/%m/%y")
## 1.c Afficher l'évolution du nombre de commentaires publiés par jour
##     Comment expliquer les pics ?
setDT(base)
ggplot(base, aes(Date)) + geom_bar()
##     Retirer les doublons et les trolls.
base <- distinct(base,authorName,text, .keep_all = TRUE)
## 1.d Déterminer quel compte a publié le plus de commentaires
donnees <- base[,.(nb_commentaire = .N), by = authorName]
donnees <- donnees[order(-nb_commentaire),]
donnees[1]$authorName #  "Dominique Descottes"
##     Combien de commentaires les gens publient-ils en moyenne
mean(donnees$nb_commentaire) # 2.041616
## 1.e Créer une variable avec la longueur du commentaire (fonction "nchar")
base$long_com = nchar(base$text)
##     Quelle est la longeur moyenne des commentaires?
mean(base$long_com) # 187.3867
##     Pensez-vous qu'il faut Supprimer les commentaires trop courts?
# oui
### II. CREATION ET EXPLORATION DU CORPUS : LEXICOMETRIE
cp <- corpus(base$text,
docvars = select(base, authorName, likeCount))
## 2.a Créer les objets tokens et la matrice termes-documents
##     Classer les mots les plus fréquents : que constatez vous?
tk<-tokens(cp, what = "word", remove_punct = TRUE, remove_numbers= T, remove_url=T)
dfm<-dfm(cp, remove=stopwords("french"), tolower=T, remove_punct=T, remove_numbers=T, remove_url = T)
topfeatures(dfm, n=100)
## 2.b Enrichir la liste des stopwords,
## puis créer à nouveau la dfm et classer les mots les plus fréquents.
dfm <- dfm_remove(dfm,c("a","c'est","ça","si","donc"))
topfeatures(dfm, n=100)
## 2.c Repérer les expressions les plus courantes et construire un dictionnaire
textstat_collocations(tk, min_count = 5, size = 2L) %>%
arrange(desc(count)) %>%
slice(1:50)
## 2.d Mots clés dans leur contexte (fonction kwic() de la librairie quanteda)
##    A quels mots sont associés les termes "police/policier" ?
##                                                 "Castaner" ?
##                                             "Gouvernement" ?
Macron<-as.data.frame(kwic(cp, pattern= "macron", window=10))
## 2.e Réaliser le nuage de mots
textplot_wordcloud(dfm,
min_size = 1,
max_size = 4,
min_count = 10,
max_words = 500,
rotation = 0.1,
color = "darkblue",
ordered_color = T)
base <- read_delim("C:/Users/brieu/Desktop/Cours/IC05/TD08/Comments.csv",
delim = ";", escape_double = FALSE, locale = locale(encoding = "WINDOWS-1252"),
trim_ws = TRUE)
## 1.b Transformer la colonne "PublishedAt" pour avoir deux variables "Date" et "Heure"
base$Date = str_sub(base$publishedAt,1,10)
base$Heure = str_sub(base$publishedAt,12,16)
##     Transformer la variable Date au format Date en utilisant le code suivant
base$Date <- as.Date(base$Date, "%d/%m/%y")
## 1.c Afficher l'évolution du nombre de commentaires publiés par jour
##     Comment expliquer les pics ?
setDT(base)
ggplot(base, aes(Date)) + geom_bar()
##     Retirer les doublons et les trolls.
base <- distinct(base,authorName,text, .keep_all = TRUE)
## 1.d Déterminer quel compte a publié le plus de commentaires
donnees <- base[,.(nb_commentaire = .N), by = authorName]
donnees <- donnees[order(-nb_commentaire),]
donnees[1]$authorName #  "Dominique Descottes"
##     Combien de commentaires les gens publient-ils en moyenne
mean(donnees$nb_commentaire) # 2.041616
## 1.e Créer une variable avec la longueur du commentaire (fonction "nchar")
base$long_com = nchar(base$text)
##     Quelle est la longeur moyenne des commentaires?
mean(base$long_com) # 187.3867
##     Pensez-vous qu'il faut Supprimer les commentaires trop courts?
# oui
### II. CREATION ET EXPLORATION DU CORPUS : LEXICOMETRIE
cp <- corpus(base$text,
docvars = select(base, authorName, likeCount))
## 2.a Créer les objets tokens et la matrice termes-documents
##     Classer les mots les plus fréquents : que constatez vous?
tk<-tokens(cp, what = "word", remove_punct = TRUE, remove_numbers= T, remove_url=T)
dfm<-dfm(cp, remove=stopwords("french"), tolower=T, remove_punct=T, remove_numbers=T, remove_url = T)
topfeatures(dfm, n=100)
## 2.b Enrichir la liste des stopwords,
## puis créer à nouveau la dfm et classer les mots les plus fréquents.
dfm <- dfm_remove(dfm,c("a","c'est","ça","si","donc","plus"))
topfeatures(dfm, n=100)
## 2.c Repérer les expressions les plus courantes et construire un dictionnaire
textstat_collocations(tk, min_count = 5, size = 2L) %>%
arrange(desc(count)) %>%
slice(1:50)
dict <- dictionary(list())
## 2.d Mots clés dans leur contexte (fonction kwic() de la librairie quanteda)
##    A quels mots sont associés les termes "police/policier" ?
##                                                 "Castaner" ?
##                                             "Gouvernement" ?
Macron<-as.data.frame(kwic(cp, pattern= "macron", window=10))
## 2.e Réaliser le nuage de mots
textplot_wordcloud(dfm,
min_size = 1,
max_size = 4,
min_count = 10,
max_words = 500,
rotation = 0.1,
color = "darkblue",
ordered_color = T)
## 1.a Charger la base
base <- read_delim("C:/Users/brieu/Desktop/Cours/IC05/TD08/Comments.csv",
delim = ";", escape_double = FALSE, locale = locale(encoding = "WINDOWS-1252"),
trim_ws = TRUE)
## 1.b Transformer la colonne "PublishedAt" pour avoir deux variables "Date" et "Heure"
base$Date = str_sub(base$publishedAt,1,10)
base$Heure = str_sub(base$publishedAt,12,16)
##     Transformer la variable Date au format Date en utilisant le code suivant
base$Date <- as.Date(base$Date, "%d/%m/%y")
## 1.c Afficher l'évolution du nombre de commentaires publiés par jour
##     Comment expliquer les pics ?
setDT(base)
ggplot(base, aes(Date)) + geom_bar()
##     Retirer les doublons et les trolls.
base <- distinct(base,authorName,text, .keep_all = TRUE)
## 1.d Déterminer quel compte a publié le plus de commentaires
donnees <- base[,.(nb_commentaire = .N), by = authorName]
donnees <- donnees[order(-nb_commentaire),]
donnees[1]$authorName #  "Dominique Descottes"
##     Combien de commentaires les gens publient-ils en moyenne
mean(donnees$nb_commentaire) # 2.041616
## 1.e Créer une variable avec la longueur du commentaire (fonction "nchar")
base$long_com = nchar(base$text)
##     Quelle est la longeur moyenne des commentaires?
mean(base$long_com) # 187.3867
##     Pensez-vous qu'il faut Supprimer les commentaires trop courts?
# oui
### II. CREATION ET EXPLORATION DU CORPUS : LEXICOMETRIE
cp <- corpus(base$text,
docvars = select(base, authorName, likeCount))
## 2.a Créer les objets tokens et la matrice termes-documents
##     Classer les mots les plus fréquents : que constatez vous?
tk<-tokens(cp, what = "word", remove_punct = TRUE, remove_numbers= T, remove_url=T)
dfm<-dfm(cp, remove=stopwords("french"), tolower=T, remove_punct=T, remove_numbers=T, remove_url = T)
topfeatures(dfm, n=100)
## 2.b Enrichir la liste des stopwords,
## puis créer à nouveau la dfm et classer les mots les plus fréquents.
dfm <- dfm_remove(dfm,c("a","c'est","ça","si","donc","plus","tout","comme",
"quand",))
topfeatures(dfm, n=100)
## 2.c Repérer les expressions les plus courantes et construire un dictionnaire
textstat_collocations(tk, min_count = 5, size = 2L) %>%
arrange(desc(count)) %>%
slice(1:50)
dict <- dictionary(list())
## 2.d Mots clés dans leur contexte (fonction kwic() de la librairie quanteda)
##    A quels mots sont associés les termes "police/policier" ?
##                                                 "Castaner" ?
##                                             "Gouvernement" ?
Macron<-as.data.frame(kwic(cp, pattern= "macron", window=10))
## 2.e Réaliser le nuage de mots
textplot_wordcloud(dfm,
min_size = 1,
max_size = 4,
min_count = 10,
max_words = 500,
rotation = 0.1,
color = "darkblue",
ordered_color = T)
base <- read_delim("C:/Users/brieu/Desktop/Cours/IC05/TD08/Comments.csv",
delim = ";", escape_double = FALSE, locale = locale(encoding = "WINDOWS-1252"),
trim_ws = TRUE)
## 1.b Transformer la colonne "PublishedAt" pour avoir deux variables "Date" et "Heure"
base$Date = str_sub(base$publishedAt,1,10)
base$Heure = str_sub(base$publishedAt,12,16)
##     Transformer la variable Date au format Date en utilisant le code suivant
base$Date <- as.Date(base$Date, "%d/%m/%y")
## 1.c Afficher l'évolution du nombre de commentaires publiés par jour
##     Comment expliquer les pics ?
setDT(base)
ggplot(base, aes(Date)) + geom_bar()
##     Retirer les doublons et les trolls.
base <- distinct(base,authorName,text, .keep_all = TRUE)
## 1.d Déterminer quel compte a publié le plus de commentaires
donnees <- base[,.(nb_commentaire = .N), by = authorName]
donnees <- donnees[order(-nb_commentaire),]
donnees[1]$authorName #  "Dominique Descottes"
##     Combien de commentaires les gens publient-ils en moyenne
mean(donnees$nb_commentaire) # 2.041616
## 1.e Créer une variable avec la longueur du commentaire (fonction "nchar")
base$long_com = nchar(base$text)
##     Quelle est la longeur moyenne des commentaires?
mean(base$long_com) # 187.3867
##     Pensez-vous qu'il faut Supprimer les commentaires trop courts?
# oui
### II. CREATION ET EXPLORATION DU CORPUS : LEXICOMETRIE
cp <- corpus(base$text,
docvars = select(base, authorName, likeCount))
## 2.a Créer les objets tokens et la matrice termes-documents
##     Classer les mots les plus fréquents : que constatez vous?
tk<-tokens(cp, what = "word", remove_punct = TRUE, remove_numbers= T, remove_url=T)
dfm<-dfm(cp, remove=stopwords("french"), tolower=T, remove_punct=T, remove_numbers=T, remove_url = T)
topfeatures(dfm, n=100)
## 2.b Enrichir la liste des stopwords,
## puis créer à nouveau la dfm et classer les mots les plus fréquents.
dfm <- dfm_remove(dfm,c("a","c'est","ça","si","donc","plus","tout","comme",
"quand"))
topfeatures(dfm, n=100)
## 2.c Repérer les expressions les plus courantes et construire un dictionnaire
textstat_collocations(tk, min_count = 5, size = 2L) %>%
arrange(desc(count)) %>%
slice(1:50)
dict <- dictionary(list())
## 2.d Mots clés dans leur contexte (fonction kwic() de la librairie quanteda)
##    A quels mots sont associés les termes "police/policier" ?
##                                                 "Castaner" ?
##                                             "Gouvernement" ?
Macron<-as.data.frame(kwic(cp, pattern= "macron", window=10))
## 2.e Réaliser le nuage de mots
textplot_wordcloud(dfm,
min_size = 1,
max_size = 4,
min_count = 10,
max_words = 500,
rotation = 0.1,
color = "darkblue",
ordered_color = T)
## 2.e Réaliser le nuage de mots
textplot_wordcloud(dfm,
min_size = 1,
max_size = 4,
min_count = 20,
max_words = 500,
rotation = 0.1,
color = "darkblue",
ordered_color = T)
## 2.e Réaliser le nuage de mots
textplot_wordcloud(dfm,
min_size = 1,
max_size = 4,
min_count = 30,
max_words = 500,
rotation = 0.1,
color = "darkblue",
ordered_color = T)
View(Macron)
library(webdriver)
install_phantomjs()
pjs_instance <- run_phantomjs()
pjs_instance <- run_phantomjs()
View(pjs_instance)
pjs_session <- Session$new(port = pjs_instance$port)
View(pjs_session)
url <- "https://webapplis.utc.fr/uvs/index.xhtml"
require("rvest")
library("rvest")
require("rvest")
# load URL to phantomJS session
pjs_session$go(url)
url <- "https://webapplis.utc.fr/uvs/index.xhtml"
require("rvest")
# load URL to phantomJS session
pjs_session$go(url)
pjs_instance <- run_phantomjs()
pjs_session <- Session$new(port = pjs_instance$port)
# load URL to phantomJS session
pjs_session$go(url)
# retrieve the rendered source code of the page
rendered_source <- pjs_session$getSource()
# parse the dynamically rendered source code
html_document <- read_html(rendered_source)
View(html_document)
title_xpath <- "//table[@style]//td[1]"
title_text <- html_document %>%
html_node(xpath = title_xpath) %>%
html_text(trim = T)
cat(title_text)
shiny::runApp('C:/Users/brieu/Desktop/Cours/TX/GIT_TX_P22/CODES/structure_shiny')
setwd("C:/Users/brieu/Desktop/Cours/TX/GIT_TX_P22/CODES/structure_shiny")
library(shiny)
library(shinydashboard)
library(readr)
library(dplyr)
library(leaflet)
library(stringr)
library(data.table)
library(rgdal)
library(ggplot2)
library(plotly)
setwd("C:/Users/brieu/Desktop/Cours/TX/GIT_TX_P22/CODES/structure_shiny")
Resultats = read_rds("../cartes/fichiers_RDS/resultats_1er_tour_2022.rds")
View(Resultats)
shiny::runApp()
runApp()
library(FactoMineR)
library(factoextra)
library(data.table)
library(corrplot)
library(ggplot2)
library(dplyr)
library(shiny)
library(FactoInvestigate)
#library(Factoshiny)
library(candisc)
library(MASS)
# chargement des données --------------------------------------------------
emploi_2017 <- readRDS("../../DATA/emploi_2017.rds")
logement_2017_2_relatif <- readRDS("../../DATA/logement_2017_2_relatif.rds")
prix_m2_commune <- readRDS("../../DATA/prix_m2_commune.rds")
resultats_t2_2017 <- readRDS("../../DATA/resultats2_2017.rds")
resultats_t1_2022 <- readRDS("../../DATA/resultats_t1_P2022.rds")
resultats_t2_2022 = read_rds("../../DATA/resultats_T2_2022.rds")
library(FactoMineR)
library(factoextra)
library(data.table)
library(corrplot)
library(ggplot2)
library(dplyr)
library(shiny)
library(FactoInvestigate)
#library(Factoshiny)
library(candisc)
library(MASS)
# chargement des données --------------------------------------------------
emploi_2017 <- readRDS("../../DATA/emploi_2017.rds")
logement_2017_2_relatif <- readRDS("../../DATA/logement_2017_2_relatif.rds")
prix_m2_commune <- readRDS("../../DATA/prix_m2_commune.rds")
resultats_t2_2017 <- readRDS("../../DATA/resultats2_2017.rds")
resultats_t1_2022 <- readRDS("../../DATA/resultats_t1_P2022.rds")
resultats_t2_2022 = readRDS("../../DATA/resultats_T2_2022.rds")
rfr_commune <- readRDS("../../DATA/rfr_2019_commune.rds")
popu_commune <- readRDS("../../DATA/popu_commune_2019.rds")
res_t1_2022_unmelted = resultats_t1_2022
setDT(res_t1_2022_unmelted)
res_t1_2022_unmelted = dcast(data = res_t1_2022_unmelted, formula = code_commune~candidat, fun.aggregate = sum, value.var = "voix")
setDF(res_t1_2022_unmelted)
res_t1_2022_unmelted[,setdiff(names(res_t1_2022_unmelted), c("code_commune", 'abstentions', "nuls", "blancs"))] = res_t1_2022_unmelted[,setdiff(names(res_t1_2022_unmelted), c("code_commune", 'abstentions', "nuls", "blancs"))]/rowSums(res_t1_2022_unmelted[,setdiff(names(res_t1_2022_unmelted), c("code_commune", 'abstentions', "nuls", "blancs"))])
resultats_t2_2017 <- rename(resultats_t2_2017, code_commune = code)
logement_2017_2_relatif <- rename(logement_2017_2_relatif, code_commune = code_com)
dataset <- merge(resultats_t2_2017, emploi_2017, by = "code_commune", all.x = TRUE)
dataset <- merge(dataset, prix_m2_commune, by = "code_commune", all.x = TRUE)
dataset <- merge(dataset, logement_2017_2_relatif, by = "code_commune", all.x = TRUE)
dataset <- merge(dataset, rfr_commune, by = "code_commune", all.x = TRUE)
dataset <- merge(dataset, popu_commune, by = "code_commune", all.x = TRUE)
dataset <- merge(dataset, res_t1_2022_unmelted, by = "code_commune", all.x = TRUE)
row.names(dataset) = dataset$code_commune
dataset$p_blancs <- 100 * dataset$Blancs / dataset$Inscrits
dataset$p_abstentions <- 100 * dataset$Abstentions / dataset$Inscrits
dataset$p_nuls <- 100 * dataset$Nuls / dataset$Inscrits
# corrélations ------------------------------------------------------------
names(dataset)
variables_acp = c("p_chom_15_64",
#"p_CS1_15_64",
"p_CS3_15_64",
"p_CS6_15_64",
"prix_moyen_m2",
"%_av_1919",
#"%_1919_1945",
#"%_1946_1970",
#"%_1971_1990",
#"%_1991_2005",
"%_ap_2006",
"%_locataires_HLM",
"%_proprios",
"%_res_secondaires",
"%_appartements",
"RFR_2019",
"popu")
for(var in variables_acp){
dataset = dataset[!is.na(dataset[var]),]
}
for(var in c("Emmanuel MACRON")){
dataset = dataset[!is.na(dataset[var]),]
}
mcor <- cor(dataset[, variables_acp], use = "complete.obs")
melted_data <- melt(mcor)
melted_data$value <- round(melted_data$value, 2)
ggplot(data = melted_data, aes(x=Var1, y=Var2, fill=value)) +
geom_tile() +  geom_text(aes(label = value), color = "white", size = 2) +
theme(axis.text.x = element_text(angle = 90))
# ACP ---------------------------------------------------------------------
#dataset$weight <- dataset$Inscrits / sum(dataset$Inscrits)
res.pca <- PCA(dataset[, variables_acp],
scale.unit = TRUE,
ncp = 10,
graph = FALSE
#row.w =dataset$weight
)
# visualisations ----------------------------------------------------------
fviz_eig(res.pca)
fviz_pca_var(res.pca, col.var = "black")
corrplot(res.pca$var$cos2, is.corr=FALSE)
corrplot(res.pca$var$contrib, is.corr=FALSE)
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
fviz_contrib(res.pca, choice = "var", axes = 3, top = 10)
fviz_contrib(res.pca, choice = "var", axes = 4, top = 10)
# dataset$p_LePen[dataset$p_LePen <= 35 ] = 35
# dataset$p_LePen[dataset$p_LePen >= 65] = 65
#
# dataset$p_blancs[dataset$p_blancs >= 20] = 20
#
# dataset$p_abstentions[dataset$p_abstentions >= 30] = 30
dataset$gagnant_hors_abs = apply(dataset[, setdiff(names(res_t1_2022_unmelted), c("code_commune", 'abstentions', "nuls", "blancs"))], 1, which.max)
dataset$gagnant_hors_abs = setdiff(names(res_t1_2022_unmelted), c("code_commune", 'abstentions', "nuls", "blancs"))[dataset$gagnant_hors_abs]
fviz_pca_ind(res.pca, col.ind=dataset$gagnant_hors_abs, axes = c(1, 2), geom.ind = c("point")) +
geom_point(aes(shape = dataset$gagnant_hors_abs, color = dataset$gagnant_hors_abs), size=0.01) +
scale_shape_manual(values=seq(15,24)) +
scale_color_manual(values=c("#404040", "#ffeb00", "#0D378A", "#dd0000", "#cc6666", "#cc2443", "#8040C0", "#dddddd", "#0066cc"))
View(resultats_t1_2022)
View(resultats_t2_2022)
View(resultats_t2_2022)
View(Resultats)
View(res_t1_2022_unmelted)
View(dataset)
dataset = merge(dataset, resultats_t2_2022[, c("code_commune", "Vainqueur")], by = "code_commune", all.x = TRUE)
fviz_pca_ind(res.pca, col.ind=dataset$Vainqueur, axes = c(1, 2), geom.ind = c("point")) +
geom_point(aes(shape = dataset$Vainqueur, color = dataset$Vainqueur), size=0.01) +
scale_shape_manual(values=seq(15,24)) +
scale_color_manual(values=c("#404040", "#ffeb00", "#0D378A", "#dd0000", "#cc6666", "#cc2443", "#8040C0", "#dddddd", "#0066cc"))
runApp()
library(rsconnect)
rsconnect::setAccountInfo(name='jeanmariegrall', token='43558D68FC8C7BD3D8AF7F722736ADD2', secret='k4xOjIKQl0xuRL7VUhgi/FNdmN2cbt5xd3HdsR4+')
deployApp()
deployApp()
runApp()
deployApp()
